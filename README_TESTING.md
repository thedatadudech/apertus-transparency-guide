# ğŸ‡¨ğŸ‡­ Swiss German AI Testing Scripts

Zwei Test-Scripts um die verschiedenen Modelle auf ihre Schweizerdeutsch-FÃ¤higkeiten zu testen.

## ğŸ“‹ Scripts Ãœbersicht

### 1. `quick_tokenizer_test.py` - Schnelle Tokenizer-Analyse
**âš¡ Schnell und lightweight**
- Nur Tokenizer-Loading (keine Models)
- Vergleicht 5+ verschiedene Tokenizer
- Zeigt Effizienz und Probleme
- LÃ¤uft auch auf CPU in ~30 Sekunden

```bash
python quick_tokenizer_test.py
```

### 2. `test_swiss_german_generation.py` - VollstÃ¤ndige Text-Generation
**ğŸ§  Komplett aber ressourcenintensiv**
- LÃ¤dt komplette Models 
- Echte Text-Generation
- Speichert Ergebnisse als JSON
- Braucht GPU fÃ¼r groÃŸe Models

```bash
python test_swiss_german_generation.py
```

## ğŸ¯ Was getestet wird

### Test-Texte:
- **Swiss German 1**: `"GrÃ¼ezi! ChÃ¶nd Sie mer bitte d Schwyzer KI erchlÃ¤re?"`
- **Swiss German 2**: `"Was isch KI und wie funktioniert das?"`
- **Standard German**: `"Hallo! KÃ¶nnen Sie mir bitte die Schweizer KI erklÃ¤ren?"`
- **Swiss Dialect**: `"Mir hÃ¤nd hÃ¼t es schÃ¶ns WÃ¤tter, gÃ¤ll?"`
- **Technical German**: `"Die KÃ¼nstliche Intelligenz verwendet neuronale Netzwerke."`

### Modelle:
- ğŸ‡¨ğŸ‡­ **Apertus Swiss AI** (`swiss-ai/Apertus-8B-Instruct-2509`)
- ğŸ‡©ğŸ‡ª **German BERT** (`bert-base-german-cased`)
- ğŸ‡©ğŸ‡ª **German GPT-2** (`dbmdz/german-gpt2`)
- ğŸŒ **Multilingual BERT** (`bert-base-multilingual-cased`)
- ğŸ¤– **Standard GPT-2** (`gpt2`)

## ğŸ“Š Was analysiert wird

### Tokenizer-QualitÃ¤t:
- **Tokens pro Zeichen** (niedriger = effizienter)
- **UTF-8 Encoding Probleme** (`ÃƒÂ¼`, `ÃƒÂ¶`, `ÃƒÂ¤`)
- **Einzelzeichen-Tokens** (ineffizient)
- **Morphologie-Splits** (Compound-Behandlung)

### Text-Generation QualitÃ¤t:
- **Schweizerdeutsch AuthentizitÃ¤t**
- **Grammatikalische Korrektheit**
- **Kulturelle Angemessenheit**
- **Generierungs-Geschwindigkeit**

## ğŸš€ Empfohlener Ablauf

### Schritt 1: Quick Test
```bash
# Schneller Ãœberblick Ã¼ber alle Tokenizer
python quick_tokenizer_test.py
```

### Schritt 2: Detaillierte Tests (wenn GPU verfÃ¼gbar)
```bash
# VollstÃ¤ndige Generation-Tests
python test_swiss_german_generation.py
```

### Schritt 3: Remote Server Test
```bash
# Auf dem Remote Server mit GPU
ssh apertus
cd /workspace/apertus-transparency-guide
source .venv/bin/activate
python test_swiss_german_generation.py
```

## ğŸ“ Output Files

### `quick_tokenizer_test.py`:
- Console Output mit Rankings
- Detaillierte Token-AufschlÃ¼sselung

### `test_swiss_german_generation.py`:
- JSON File: `swiss_german_test_results_YYYYMMDD_HHMMSS.json`
- EnthÃ¤lt alle Generationen, Timings, Fehler

## ğŸ” Interpretation der Ergebnisse

### Tokenizer Rankings:
- **Niedriger tok/char Ratio** = effizienter
- **Wenig "Ãƒ" tokens** = bessere UTF-8 Behandlung
- **Wenig Einzelzeichen** = bessere Compound-Behandlung

### Generation Quality:
- **Authentisches Schweizerdeutsch** vs. Standard Deutsch
- **Konsistente Grammatik**
- **Kulturell angemessene Begriffe**

## âš ï¸ Hardware Requirements

### Quick Test:
- âœ… CPU only
- âœ… 4GB RAM minimum
- âœ… ~2GB Download (Tokenizer)

### Full Test:
- ğŸ® GPU empfohlen (8GB+ VRAM)
- ğŸ’¾ 16GB+ RAM  
- ğŸ“¦ ~30GB Download (alle Models)

## ğŸ› Troubleshooting

### "Model zu groÃŸ" Fehler:
```python
# In test_swiss_german_generation.py, reduziere max_new_tokens:
max_new_tokens=50  # statt 150
```

### UTF-8 Probleme:
```bash
export PYTHONIOENCODING=utf-8
export LANG=en_US.UTF-8
```

### Memory Errors:
```python
# Verwende kleinere batch size oder float32 statt float16
torch_dtype=torch.float32
```

## ğŸ“ˆ Beispiel Output

```
ğŸ¥‡ German BERT        : 0.324 tok/char, 35 tokens, 2 problems
ğŸ¥ˆ Apertus Swiss AI   : 0.315 tok/char, 34 tokens, 6 problems  
ğŸ¥‰ German GPT-2       : 0.306 tok/char, 33 tokens, 9 problems
4. Multilingual BERT  : 0.361 tok/char, 39 tokens, 3 problems
```

Das zeigt: **German BERT** ist am effizientesten mit wenigsten Problemen, aber **Apertus** ist Ã¼berraschend gut bei Token-Effizienz!